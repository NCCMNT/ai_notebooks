{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7468d0c",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor Notebook\n",
    "### Author: Krzysztof Chmielewski\n",
    "\n",
    "This notebook implements a gradient boosting regressor from scratch and evaluates it on the Boston housing dataset. It demonstrates:\n",
    "- Sequential tree building with residual fitting\n",
    "- Gradient boosting algorithm (additive ensemble, shrinkage)\n",
    "- Regression tree integration for boosting\n",
    "- Training and evaluation (RMSE, MAE, R² metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa9f5f",
   "metadata": {},
   "source": [
    "## Model Purpose and Applications\n",
    "\n",
    "**Gradient Boosting** is a powerful ensemble algorithm that sequentially builds weak learners (typically shallow trees), each correcting the residual errors of all previous trees. It often achieves state-of-the-art performance on regression and classification problems.\n",
    "\n",
    "### Key Use Cases:\n",
    "- **Kaggle competitions**: Often the top-performing model for structured data\n",
    "- **Housing price prediction**: High-accuracy real estate valuation\n",
    "- **Demand forecasting**: Predicting product demand with complex patterns\n",
    "- **Risk modeling**: Financial and credit risk assessment\n",
    "- **Click-through rate (CTR) prediction**: Online advertising performance prediction\n",
    "- **Ranking problems**: Learning-to-rank for search engines\n",
    "- **Medical prognosis**: Patient outcome prediction with high accuracy\n",
    "- **Energy consumption forecasting**: Complex demand patterns\n",
    "\n",
    "### Strengths:\n",
    "- **Highest accuracy** — often achieves state-of-the-art results on structured data\n",
    "- **Sequential learning** — each tree corrects previous errors (residuals)\n",
    "- **Robust to outliers** — iterative error correction handles anomalies\n",
    "- Handles complex non-linear relationships\n",
    "- Can work with missing data (with modifications)\n",
    "- Provides feature importance rankings\n",
    "- Flexible loss functions (MSE, MAE, custom)\n",
    "\n",
    "### Limitations:\n",
    "- **Requires careful tuning** — many hyperparameters (learning rate, n_trees, max_depth)\n",
    "- **Slow to train** — sequential nature prevents parallelization across trees\n",
    "- Prone to **overfitting** if not carefully regulated\n",
    "- Difficult to interpret compared to single trees\n",
    "- Sensitive to learning rate and stopping criteria\n",
    "- Computationally expensive for very large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837a44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.evaluation import eval_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37ee46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y):\n",
    "    if len(y) == 0: return 0\n",
    "    return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    left_mask = X[:, feature] <= threshold\n",
    "    right_mask = X[:, feature] > threshold\n",
    "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47eb989",
   "metadata": {},
   "source": [
    "## Gradient Boosting: Core Concepts\n",
    "\n",
    "Gradient Boosting builds an additive ensemble by iteratively fitting weak learners (regression trees) to the **residuals** of the previous models.\n",
    "\n",
    "### Initial Prediction\n",
    "\n",
    "Start with an initial regression tree:\n",
    "\n",
    "$$\\hat{y}^{(0)} = \\text{Tree}_0(X)$$\n",
    "\n",
    "### Sequential Residual Fitting\n",
    "\n",
    "For each iteration $m = 1, 2, \\ldots, M$:\n",
    "\n",
    "1. Compute residuals (errors) of current predictions:\n",
    "   $$r_m = y - \\hat{y}^{(m-1)}$$\n",
    "\n",
    "2. Fit a new tree to these residuals:\n",
    "   $$\\text{Tree}_m = \\text{fit}(X, r_m)$$\n",
    "\n",
    "3. Update predictions with shrinkage (learning rate $\\eta$):\n",
    "   $$\\hat{y}^{(m)} = \\hat{y}^{(m-1)} + \\eta \\cdot \\text{Tree}_m(X)$$\n",
    "\n",
    "The learning rate $\\eta$ (typically 0.01-0.2) controls the contribution of each new tree, preventing overfitting.\n",
    "\n",
    "### Final Prediction\n",
    "\n",
    "$$\\hat{y} = \\text{Tree}_0(X) + \\eta \\cdot \\sum_{m=1}^{M} \\text{Tree}_m(X)$$\n",
    "\n",
    "This additive ensemble approach allows Gradient Boosting to iteratively reduce prediction error, often achieving superior performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244bc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree():\n",
    "\n",
    "    def __init__(self, max_depth = 5, min_samples_split = 2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "            self.feature = feature\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value \n",
    "\n",
    "    def __best_split(self, X, y):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_mse = np.inf\n",
    "\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:,feature])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                _, y_left, _, y_right = split_dataset(X,y,feature,threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0: continue\n",
    "\n",
    "                mse_left = MSE(y_left)\n",
    "                mse_right = MSE(y_right)\n",
    "\n",
    "                weighted_mse = (len(y_left)/len(y) * mse_left + len(y_right)/len(y) * mse_right)\n",
    "\n",
    "                if weighted_mse < best_mse:\n",
    "                    best_mse = weighted_mse\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_mse\n",
    "    \n",
    "    def __build_tree(self, X, y, depth=0):\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split:\n",
    "            return self.Node(value=np.mean(y))\n",
    "        \n",
    "        feature, threshold, mse = self.__best_split(X,y)\n",
    "\n",
    "        if feature is None or mse >= MSE(y):\n",
    "            return self.Node(value=np.mean(y))\n",
    "        \n",
    "        X_left, y_left, X_right, y_right = split_dataset(X, y, feature, threshold)\n",
    "\n",
    "        left_child = self.__build_tree(X_left, y_left, depth+1)\n",
    "        right_child = self.__build_tree(X_right, y_right, depth+1)\n",
    "\n",
    "        return self.Node(feature, threshold, left_child, right_child)\n",
    "    \n",
    "    def __predict_sample(self, x, node: Node):\n",
    "        if node.left is None and node.right is None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.__predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self.__predict_sample(x, node.right)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self.__build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_sample(x, self.root) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5682faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost():\n",
    "    def __init__(self, lr=0.01, n_trees=10, max_depth=5, min_samples_split=2):\n",
    "        self.lr = lr\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.init_tree = RegressionTree(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split\n",
    "        )\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        self.init_tree.fit(X,y)\n",
    "        y_pred = self.init_tree.predict(X)\n",
    "\n",
    "        for i in range(self.n_trees):\n",
    "            residuals = y - y_pred\n",
    "            new_tree = RegressionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            new_tree.fit(X, residuals)\n",
    "            y_pred += self.lr * new_tree.predict(X)\n",
    "\n",
    "            self.trees.append(new_tree)\n",
    "\n",
    "            if verbose and i % (self.n_trees // 10) == 0:\n",
    "                loss = np.sqrt(np.mean((y - y_pred)**2))\n",
    "                print(f\"Tree {i}, RMSE: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = self.init_tree.predict(X)\n",
    "        for tree in self.trees:\n",
    "            pred += self.lr * tree.predict(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd12139",
   "metadata": {},
   "source": [
    "## Boston Housing Dataset\n",
    "\n",
    "The Boston housing dataset contains information about housing in the Boston area with 506 samples and 13 features. The target variable `MEDV` is the median home value in $1000s. This dataset is commonly used for regression benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fbf3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B-1000</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO  B-1000  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/boston/boston.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c2409",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Split the data into training and test sets and build the gradient boosting ensemble.\n",
    "\n",
    "- Train/test split: `train_test_split(X, Y, test_size=0.3, random_state=42)` for standard regression split\n",
    "- Hyperparameters: `lr=0.2` (learning rate), `n_trees=50` (number of boosting iterations), `max_depth=5` (max tree depth)\n",
    "- Each tree is trained on residuals from previous predictions\n",
    "- Sequential nature: each tree corrects errors of the ensemble so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39b163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"MEDV\", axis=1).values\n",
    "Y = data[\"MEDV\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7f41dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0, RMSE: 2.3348\n",
      "Tree 5, RMSE: 1.8439\n",
      "Tree 10, RMSE: 1.5207\n",
      "Tree 15, RMSE: 1.2754\n",
      "Tree 20, RMSE: 1.0589\n",
      "Tree 25, RMSE: 0.9180\n",
      "Tree 30, RMSE: 0.7403\n",
      "Tree 35, RMSE: 0.6342\n",
      "Tree 40, RMSE: 0.5365\n",
      "Tree 45, RMSE: 0.4678\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoost(lr=0.2, n_trees=50)\n",
    "GB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039c919",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training, we predict continuous values on the test set and evaluate using regression metrics:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2}$ — penalizes large errors\n",
    "- **MAE (Mean Absolute Error)**: $\\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i|$ — robust to outliers\n",
    "- **R² Score**: $1 - \\frac{\\sum (\\hat{y}_i - y_i)^2}{\\sum (y_i - \\bar{y})^2}$ — proportion of variance explained (0 to 1)\n",
    "\n",
    "We use the local helper `eval_regression(y_true, y_pred)` to compute and display these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b745b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 13.237\n",
      "RMSE: 3.638\n",
      "MAE: 2.429\n",
      "R2: 0.822\n"
     ]
    }
   ],
   "source": [
    "y_pred = GB.predict(X_test)\n",
    "eval_regression(y_test, y_pred);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
